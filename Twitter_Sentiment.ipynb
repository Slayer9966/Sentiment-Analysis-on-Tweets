{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7eebb3-a18d-4e45-845a-705712dfcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10760dd3-8ffc-44f3-904d-7f49a1290790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model without the optimizer\n",
    "model = keras.models.load_model(\"best_bilstm_model.keras\", compile=False)\n",
    "\n",
    "# Recompile with the same optimizer (optional, if you plan to retrain)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcc5a23-3648-475a-9c94-62e6d136e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT embeddings\n",
    "with open(\"bert_embeddings.pkl\", \"rb\") as f:\n",
    "    bert_embeddings = pickle.load(f)  # Load actual BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e08f119-48c1-4485-a560-dd2a65c6af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same BERT model used for training\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a6a2a-8216-427f-91b4-d2306a21b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"\"\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be763027-7b0e-4556-9fce-b50f6e25aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tweepy Client\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f14142-8223-4947-b975-0d93e3b5db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions (@user)\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters & punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c677ae4-5220-4c24-944b-37774b979f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    \"\"\"Convert a sentence into its BERT embedding.\"\"\"\n",
    "    words = text.split()\n",
    "    word_vectors = [bert_embeddings[word] for word in words if word in bert_embeddings]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return np.zeros((768,))  # Return zero vector if no words found\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce5506b-7675-49bf-8762-2526967d790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweets_and_replies(keyword, num_tweets=10):\n",
    "    tweets_data = []\n",
    "\n",
    "    try:\n",
    "        # Fetch recent tweets\n",
    "        tweets = client.search_recent_tweets(query=keyword, max_results=num_tweets, tweet_fields=[\"conversation_id\", \"author_id\"])\n",
    "\n",
    "        if tweets.data:\n",
    "            for tweet in tweets.data:\n",
    "                conversation_id = tweet.id\n",
    "                tweet_text = clean_text(tweet.text)  # Clean tweet text\n",
    "                replies = []\n",
    "                \n",
    "                # Fetch replies for the tweet\n",
    "                reply_query = f\"conversation_id:{conversation_id} -is:retweet\"\n",
    "                \n",
    "                try:\n",
    "                    reply_tweets = client.search_recent_tweets(query=reply_query, max_results=5)\n",
    "                    if reply_tweets.data:\n",
    "                        replies = [clean_text(reply.text) for reply in reply_tweets.data]  # Clean replies\n",
    "                except tweepy.TooManyRequests:\n",
    "                    print(\"Rate limit exceeded while fetching replies. Waiting for 15 minutes...\")\n",
    "                    time.sleep(900)  # Wait for 15 minutes\n",
    "                    return fetch_tweets_and_replies(keyword, num_tweets)\n",
    "\n",
    "                tweets_data.append({\"tweet\": tweet_text, \"replies\": replies})\n",
    "\n",
    "    except tweepy.TooManyRequests:\n",
    "        print(\"Rate limit exceeded. Waiting for 15 minutes...\")\n",
    "        time.sleep(900)  # Wait for 15 minutes\n",
    "        return fetch_tweets_and_replies(keyword, num_tweets)\n",
    "\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34fec410-0d87-40c1-94b2-3f37bfb8e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comments(comments):\n",
    "    \"\"\"Convert comments into BERT embeddings and predict sentiment using BiLSTM model.\"\"\"\n",
    "    processed_comments = [clean_text(comment) for comment in comments]\n",
    "    embeddings = np.array([get_bert_embedding(comment) for comment in processed_comments])\n",
    "    \n",
    "    predictions = model.predict(embeddings)  # Get model predictions\n",
    "    sentiment_counts = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if pred >= 0.6:  # Positive sentiment\n",
    "            sentiment_counts[\"positive\"] += 1\n",
    "        elif pred <= 0.4:  # Negative sentiment\n",
    "            sentiment_counts[\"negative\"] += 1\n",
    "        else:  # Neutral sentiment\n",
    "            sentiment_counts[\"neutral\"] += 1\n",
    "    \n",
    "    total = sum(sentiment_counts.values())\n",
    "    sentiment_percentages = {k: round((v / total) * 100, 2) for k, v in sentiment_counts.items()}\n",
    "\n",
    "    return sentiment_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49339c-bd88-4000-862e-0c5d9c69dcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Waiting for 15 minutes...\n"
     ]
    }
   ],
   "source": [
    "# Fetch tweets and classify their sentiment\n",
    "keyword = \"AI technology\"  # Change keyword\n",
    "tweets = fetch_tweets_and_replies(keyword)\n",
    "\n",
    "if not tweets:\n",
    "    print(\"No tweets found.\")\n",
    "else:\n",
    "    all_comments = []\n",
    "    for tweet in tweets:\n",
    "        all_comments.append(tweet[\"tweet\"])\n",
    "        all_comments.extend(tweet[\"replies\"])\n",
    "\n",
    "    if all_comments:\n",
    "        sentiment_result = classify_comments(all_comments)\n",
    "\n",
    "        # Display Sentiment Analysis Result\n",
    "        print(\"\\nSentiment Breakdown:\")\n",
    "        for sentiment, percentage in sentiment_result.items():\n",
    "            print(f\"{sentiment.capitalize()}: {percentage}%\")\n",
    "    else:\n",
    "        print(\"No comments to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b66ffa-57b9-446b-bee9-8816e2430c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
